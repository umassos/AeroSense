{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b7c52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"frame will be truncated. Increase NFFT to avoid.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import python_speech_features as speech_feat\n",
    "import glob\n",
    "import wave\n",
    "from numpy.fft import fft, ifft, fft2, ifft2, fftshift\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter, filtfilt, freqz\n",
    "from dtaidistance import dtw\n",
    "import pickle\n",
    "from scipy import signal\n",
    "\n",
    "# from project_strings import *\n",
    "# import audio_processing as ap\n",
    "# import opensmile\n",
    "\n",
    "# template_R_X_a = pd.read_csv('templates/budsLive-focused-new/template_R_X_a_f_hfp.csv', header=None)[0]\n",
    "# template_R_Y_a = pd.read_csv('templates/budsLive-focused-new/template_R_Y_a_f_hfp.csv', header=None)[0]\n",
    "# template_R_Z_a = pd.read_csv('templates/budsLive-focused-new/template_R_Z_a_f_hfp.csv', header=None)[0]\n",
    "# template_L_X_a = pd.read_csv('templates/budsLive-focused-new/template_L_X_a_f_hfp.csv', header=None)[0]\n",
    "# template_L_Y_a = pd.read_csv('templates/budsLive-focused-new/template_L_Y_a_f_hfp.csv', header=None)[0]\n",
    "# template_L_Z_a = pd.read_csv('templates/budsLive-focused-new/template_L_Z_a_f_hfp.csv', header=None)[0]\n",
    "\n",
    "class Device_Features:\n",
    "    def __init__(self, slice, sampling_rate, magnitudes, freqs):\n",
    "        self.data = slice\n",
    "        self.sr = sampling_rate\n",
    "        self.magnitudes =  magnitudes\n",
    "        self.freqs = freqs\n",
    "\n",
    "    def zcr(self):\n",
    "        ans = 0\n",
    "        signalDuration = float(len(self.data)) / float(self.sr)\n",
    "        for i in range(1, len(self.data)):\n",
    "            if self.data[i] * self.data[i - 1] < 0:\n",
    "                ans += 1\n",
    "        ans /= signalDuration\n",
    "        return ans\n",
    "\n",
    "    def SPlevel(self):\n",
    "        data = self.data\n",
    "        energy = 0\n",
    "        for value in data:\n",
    "            energy = energy + (float(value)*float(value))\n",
    "        power = 20 * math.log10((math.sqrt(energy)+0.0001) / float(len(data)))\n",
    "        return power\n",
    "\n",
    "    def Spec_Cent(self):\n",
    "        magnitudes = self.magnitudes\n",
    "        freqs = self.freqs\n",
    "        return np.sum(magnitudes * freqs) / np.sum(magnitudes)\n",
    "\n",
    "    def Spec_Spread(self):\n",
    "        magnitudes = self.magnitudes\n",
    "        freqs = self.freqs\n",
    "        SC = self.Spec_Cent()\n",
    "        M2 = np.sum(magnitudes * (freqs**2)) / np.sum(magnitudes)\n",
    "        if np.sum(magnitudes) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return math.sqrt(M2 - SC**2)\n",
    "\n",
    "    def Spec_Rolloff(self):\n",
    "        magnitudes = self.magnitudes\n",
    "        sr = self.sr\n",
    "        spectralSum = 0\n",
    "        for i in magnitudes:\n",
    "            spectralSum += i\n",
    "        rolloffSum = 0\n",
    "        rolloffIndex = 0\n",
    "        for index, value in enumerate(magnitudes):\n",
    "            rolloffSum += value\n",
    "            if rolloffSum > 0.85*spectralSum:\n",
    "                rolloffIndex = index\n",
    "                break\n",
    "        return rolloffIndex*((sr/2.0)/len(magnitudes))\n",
    "\n",
    "    def Spec_Flatness(self):\n",
    "        magnitudes = self.magnitudes\n",
    "        sr = self.sr\n",
    "        num = 0\n",
    "        den = 0\n",
    "\n",
    "        for i in magnitudes:\n",
    "            if i != 0:\n",
    "                num += math.log(i)\n",
    "                den += i\n",
    "        if den != 0:\n",
    "            return math.exp(num/float(len(magnitudes)))/(den/float(len(magnitudes)))\n",
    "        else:\n",
    "            return math.exp(1/float(len(magnitudes)))/(1/float(len(magnitudes)))\n",
    "\n",
    "    def Quartile_Range(self):\n",
    "        data = self.data\n",
    "        data = np.abs(data)\n",
    "\n",
    "        return np.quantile(data, 0.75) - np.quantile(data, 0.25)\n",
    "\n",
    "    def Chroma(self):\n",
    "        data = self.data\n",
    "        magnitudes = self.magnitudes\n",
    "        sr = self.sr\n",
    "\n",
    "        # # implementing through PyAudio\n",
    "        # nFFT = int(len(data))  # half of window_size\n",
    "        # nChroma, nFreqsPerChroma = audioFeatureExtraction.stChromaFeaturesInit(nFFT, sr)\n",
    "        # chroma_feat = audioFeatureExtraction.stChromaFeatures(data, sr, nChroma, nFreqsPerChroma)\n",
    "\n",
    "        # copying the java library\n",
    "        chroma = np.zeros(12)\n",
    "        from audiolazy.lazy_midi import freq2midi\n",
    "        df = (sr/2.0)/len(magnitudes)\n",
    "        for index, value in enumerate(magnitudes):\n",
    "            frequencyi = df*index\n",
    "            if frequencyi == 0:\n",
    "                pitch = 0\n",
    "            else:\n",
    "                pitch = freq2midi(frequencyi)\n",
    "\n",
    "            pitchClass = int(pitch%12)\n",
    "            if pitchClass < 0:\n",
    "                pitchClass += 12\n",
    "            chroma[pitchClass] += value\n",
    "\n",
    "        maxElement = np.max(chroma)\n",
    "        for index, value in enumerate(chroma):\n",
    "            chroma[index] = value/maxElement\n",
    "        return chroma.tolist()\n",
    "\n",
    "    def Energy(self):\n",
    "        data = self.data\n",
    "\n",
    "        energy = 0\n",
    "        for i in data:\n",
    "            energy += i*i\n",
    "        return energy/float(len(data))\n",
    "\n",
    "def cough_post_processing(test_file_features, save_cough_snippets = False, audio_path = None, jump_size = 0.1):\n",
    "\n",
    "    predictions = test_file_features['prediction'].tolist()\n",
    "\n",
    "    # step 0, check the weak cough test\n",
    "    for index, value in enumerate(predictions):\n",
    "        if predictions[index] == 'cough':\n",
    "            if test_file_features.iloc[index]['SPlevel'] < -77:\n",
    "                predictions[index] = 'non-cough'\n",
    "            elif test_file_features.iloc[index]['std'] < 0.011:\n",
    "                predictions[index] = 'non-cough'\n",
    "            elif test_file_features.iloc[index]['mfcc_00_std'] < 0.51:\n",
    "                predictions[index] = 'non-cough'\n",
    "\n",
    "\n",
    "    # step 1: converting a non-cough that is surrounded by cough predictions\n",
    "    for index, value in enumerate(predictions[:-2]):\n",
    "        if predictions[index] == 'cough' and predictions[index+2] == 'cough':\n",
    "            predictions[index+1] = 'cough'\n",
    "\n",
    "    # step 2:\n",
    "    cough_like_numbers = 0\n",
    "    cough_start_index, cough_stop_index = 0, 0\n",
    "    found_a_cough = False\n",
    "    post_predictions = [0] * len(predictions)\n",
    "    for index, value in enumerate(predictions):\n",
    "        if index in [0, 1, 2]:\n",
    "            continue\n",
    "        else:\n",
    "            if predictions[index] == \"cough\":\n",
    "                if found_a_cough:\n",
    "                    index += 1\n",
    "                    continue\n",
    "                elif predictions[index - 1] == \"cough\" and predictions[index - 2] == \"cough\":\n",
    "                    # you might need to add the next line if the jump size is smaller than 0.1\n",
    "                    # and predictions[index - 2] == \"cough\" and predictions[index - 3] == \"cough\":\n",
    "                    found_a_cough = True\n",
    "                    cough_like_numbers += 1\n",
    "                    cough_start_index = index - 2\n",
    "                    index += 1\n",
    "            else:\n",
    "                if found_a_cough:\n",
    "                    found_a_cough = False\n",
    "                    cough_stop_index = index\n",
    "                    post_predictions[cough_start_index:cough_stop_index] = [1] * (cough_stop_index - cough_start_index)\n",
    "                    if save_cough_snippets:\n",
    "                        sr_test, wav_test = sp.io.wavfile.read(audio_path)\n",
    "                        audio_folder = '/'.join(audio_path.split('/')[:-1])\n",
    "                        create_path(audio_folder+'/snippets')\n",
    "                        sp.io.wavfile.write(audio_folder+'/snippets/' + audio_path.split('/')[-1][:-4] + \"_cough_\" +\n",
    "                                            str(jump_size * (cough_start_index-2)) + '.wav', sr_test,\n",
    "                                            wav_test[int(jump_size * sr_test * (cough_start_index-2)):int(\n",
    "                                                jump_size * sr_test * (cough_stop_index+2))])\n",
    "                else:\n",
    "                    continue\n",
    "    return post_predictions\n",
    "\n",
    "def cross_correlation_using_fft(x, y):\n",
    "    f1 = fft(x)\n",
    "    f2 = fft(np.flipud(y))\n",
    "    cc = np.real(ifft(f1 * f2))\n",
    "    return fftshift(cc)\n",
    "\n",
    "# shift < 0 means that y starts 'shift' time steps before x # shift &gt; 0 means that y starts 'shift' time steps after x\n",
    "def compute_shift(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    c = cross_correlation_using_fft(x, y)\n",
    "    assert len(c) == len(x)\n",
    "    zero_index = int(len(x) / 2) - 1\n",
    "    shift = zero_index - np.argmax(c)\n",
    "    return shift\n",
    "\n",
    "def label_file_to_DF(txt_address):\n",
    "    with open(txt_address) as f:\n",
    "        start_time = []\n",
    "        stop_time = []\n",
    "        label = []\n",
    "        for line in f:\n",
    "            splitted = line.split()\n",
    "            if splitted[2] == 'Start':\n",
    "                start_time.append('%.4f'%float(splitted[0]))\n",
    "            elif splitted[2] == 'End':\n",
    "                stop_time.append('%.4f'%float(splitted[0]))\n",
    "                label.append('cough')\n",
    "            else:\n",
    "                start_time.append('%.4f'%float(splitted[0]))\n",
    "                stop_time.append('%.4f'%float(splitted[1]))\n",
    "                label.append(splitted[2])\n",
    "\n",
    "    label_DF = pd.DataFrame(data= {'start_times': start_time, 'stop_times': stop_time, 'label': label})\n",
    "    return label_DF\n",
    "\n",
    "\n",
    "def wave_file_to_device_feature_table_sliding(wav_path, win_size, jump_size, normalization =  False, label = None, downsampling_freq = None, hard_max = None):\n",
    "    # creating features from field data\n",
    "    sr, wav = sp.io.wavfile.read(wav_path)\n",
    "    ####################################\n",
    "    # picking the channel with higher amplitude if there is two channels\n",
    "    try:\n",
    "        if np.sum(np.absolute(np.array(wav[:, 0]))) > np.sum(np.absolute(np.array(wav[:, 1]))):\n",
    "            wav = np.array(wav[:, 0])\n",
    "            print(\"channel one\")\n",
    "        else:\n",
    "            wav = np.array(wav[:, 1])\n",
    "            print(\"channel two\")\n",
    "    except:\n",
    "        print(\"single channel file\")\n",
    "\n",
    "\n",
    "    # expanding the resolution to float so it doesn't go out of range if wav is 16 bit int\n",
    "    wav = wav.astype(int)\n",
    "\n",
    "    # downsampling the file\n",
    "    if downsampling_freq != None:\n",
    "        downsampling_ratio = int(sr / downsampling_freq)\n",
    "        wav = sp.signal.decimate(wav, downsampling_ratio)\n",
    "        sr = downsampling_freq\n",
    "\n",
    "    # normalizing the amplitude\n",
    "    if normalization:\n",
    "        if hard_max == None:\n",
    "            max_val = max(wav)\n",
    "            min_val = min(wav)\n",
    "        else:\n",
    "            max_val = max(max(wav), hard_max)\n",
    "            min_val = max(min(wav), hard_max*-1)\n",
    "        max_min_max = max(abs(max_val), abs(min_val))\n",
    "        normalization_ratio = max_min_max * 2\n",
    "        wav = (wav + max_min_max) / normalization_ratio - 0.5\n",
    "    else:\n",
    "        normalization_ratio = 1\n",
    "\n",
    "    # rounding the digits after decimal point to a certain number\n",
    "    wav = np.round(wav, decimals=5)\n",
    "    #####################################\n",
    "    feature_table = pd.DataFrame()\n",
    "    for i in range(0, int((len(wav)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav):\n",
    "            break\n",
    "        else:\n",
    "            wave_piece = wav[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "            feature_row_per_sub = wave_slice_to_device_features(wave_piece, sr)\n",
    "            if feature_table.empty:\n",
    "                feature_table = feature_row_per_sub\n",
    "            else:\n",
    "                feature_table = pd.concat([feature_table, feature_row_per_sub], axis=0)\n",
    "    if normalization:\n",
    "        feature_table['normalization_ratio'] = normalization_ratio\n",
    "\n",
    "    if label != None:\n",
    "        feature_table['label'] = label\n",
    "\n",
    "    return feature_table\n",
    "\n",
    "def normalize_DF(df):\n",
    "    x = df.values  # returns a numpy array\n",
    "    columns = df.columns.tolist()\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled, columns= columns)\n",
    "    return df\n",
    "\n",
    "def IMU_DTW_test(IMU_DF):\n",
    "    template_R_X_a = pickle.load(open('templates/manual/template_R_X_a_f', 'rb'))\n",
    "    template_R_Y_a = pickle.load(open('templates/manual/template_R_Y_a_f', 'rb'))\n",
    "    template_R_Z_a = pickle.load(open('templates/manual/template_R_Z_a_f', 'rb'))\n",
    "    template_L_X_a = pickle.load(open('templates/manual/template_L_X_a_f', 'rb'))\n",
    "    template_L_Y_a = pickle.load(open('templates/manual/template_L_Y_a_f', 'rb'))\n",
    "    template_L_Z_a = pickle.load(open('templates/manual/template_L_Z_a_f', 'rb'))\n",
    "\n",
    "    jump_size = 10\n",
    "    win_size = 50\n",
    "    distance_list = []\n",
    "    IMU_DF = IMU_DF[[' L_ACC_X_f', ' L_ACC_Y_f', ' L_ACC_Z_f', ' R_ACC_X_f', ' R_ACC_Y_f', ' R_ACC_Z_f']]\n",
    "    for i in range(0, int(IMU_DF.shape[0] / jump_size)):\n",
    "        if int(i * jump_size + win_size) > IMU_DF.shape[0]:\n",
    "            break\n",
    "        else:\n",
    "            IMU_piece = IMU_DF.iloc[int(i * jump_size):int(i * jump_size + win_size)]\n",
    "            alignment_R_X_a = dtw.distance(IMU_piece[' R_ACC_X_f'].tolist(), template_R_X_a.tolist())\n",
    "            alignment_R_Y_a = dtw.distance(IMU_piece[' R_ACC_Y_f'].tolist(), template_R_Y_a.tolist())\n",
    "            alignment_R_Z_a = dtw.distance(IMU_piece[' R_ACC_Z_f'].tolist(), template_R_Z_a.tolist())\n",
    "            alignment_L_X_a = dtw.distance(IMU_piece[' L_ACC_X_f'].tolist(), template_L_X_a.tolist())\n",
    "            alignment_L_Y_a = dtw.distance(IMU_piece[' L_ACC_Y_f'].tolist(), template_L_Y_a.tolist())\n",
    "            alignment_L_Z_a = dtw.distance(IMU_piece[' L_ACC_Z_f'].tolist(), template_L_Z_a.tolist())\n",
    "            distance_list.append((alignment_R_X_a * alignment_R_Y_a * alignment_R_Z_a * alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a)\n",
    "                                 ** (1 / float(6)))\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(distance_list)\n",
    "    # plt.ylim([0, 5000])\n",
    "    plt.savefig('distance.png')\n",
    "\n",
    "    return distance_list\n",
    "\n",
    "def process_the_chunk(top_index, chunk_size, wav_chunk, sr, IMU_chunk, enable_IMU, residual_wav, residual_IMU,\n",
    "                      model, win_size, jump_size, post_processing, feature_list, silent_threshold, IMU_template_df):\n",
    "\n",
    "\n",
    "\n",
    "    prediction_list = []\n",
    "    distance_list = []\n",
    "    wav_chunk = np.concatenate((residual_wav, wav_chunk), axis=None)\n",
    "    IMU_chunk = pd.concat([residual_IMU, IMU_chunk])\n",
    "\n",
    "\n",
    "    # looping through the chunk for prediction\n",
    "    for i in range(0, int((len(wav_chunk)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav_chunk):\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):]\n",
    "            # IMU_piece = IMU_chunk.iloc[int(i * 80 * jump_size + 80 * win_size):]\n",
    "        else:\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "\n",
    "        feature_row_per_sub = wave_slice_to_device_features(wave_piece, sr)\n",
    "        if feature_row_per_sub['SPlevel'][0] < silent_threshold:\n",
    "            prediction_list.append(0)\n",
    "            distance_list.append(10000)\n",
    "        else:\n",
    "            selected_features = feature_row_per_sub[feature_list]\n",
    "            prediction = model.predict(selected_features)[0]\n",
    "\n",
    "            if prediction == 'cough':\n",
    "                prediction_list.append(2)\n",
    "                if enable_IMU:\n",
    "                    j = 0\n",
    "                    distance = 10000\n",
    "                    while j* jump_size * 50 + 20 < len(IMU_chunk):\n",
    "                        IMU_piece = IMU_chunk.iloc[int(j* jump_size * 50):int(j* jump_size * 50 + 20)]\n",
    "                        alignment_L_X_a = dtw.distance(IMU_piece['ACC_X_f'].tolist(), IMU_template_df['acc_x'].tolist())\n",
    "                        alignment_L_Y_a = dtw.distance(IMU_piece['ACC_Y_f'].tolist(), IMU_template_df['acc_y'].tolist())\n",
    "                        alignment_L_Z_a = dtw.distance(IMU_piece['ACC_Z_f'].tolist(), IMU_template_df['acc_z'].tolist())\n",
    "                        distance = min(distance, (alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(3)))\n",
    "                        j += 1\n",
    "                    distance_list.append(distance)\n",
    "            else:\n",
    "                prediction_list.append(1)\n",
    "                distance_list.append(10000)\n",
    "\n",
    "    final_cough_results = pd.DataFrame(columns=['start_time', 'end_time', 'duration', 'intensity', 'type'])\n",
    "    if post_processing:\n",
    "        index = 0\n",
    "        cough_start_list = []\n",
    "        cough_end_list = []\n",
    "        post_prediction_list = prediction_list.copy()\n",
    "        while index < (len(prediction_list[:-3])):\n",
    "            if prediction_list[index] == 2 and prediction_list[index + 3] == 2:\n",
    "                post_prediction_list[index + 1] = 2\n",
    "                post_prediction_list[index + 2] = 2\n",
    "                index += 4\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        index = 0\n",
    "        cough_happening = False\n",
    "        while index < (len(post_prediction_list)):\n",
    "            if post_prediction_list[index] == 2 and not cough_happening:\n",
    "                if index == (len(post_prediction_list) -1):\n",
    "                    cough_start_list.append(index)\n",
    "                    cough_end_list.append(index+1)\n",
    "                    break\n",
    "                else:\n",
    "                    cough_happening = True\n",
    "                    cough_start_list.append(index)\n",
    "                    index += 1\n",
    "                    continue\n",
    "            elif cough_happening and post_prediction_list[index] != 2:\n",
    "                cough_happening = False\n",
    "                cough_end_list.append(index)\n",
    "                index += 1\n",
    "                continue\n",
    "            elif cough_happening and index == (len(post_prediction_list)-1):\n",
    "                cough_end_list.append(index+1)\n",
    "                break\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        index = 0\n",
    "        while index < len(cough_start_list):\n",
    "            cough_duration = win_size + (cough_end_list[index] - cough_start_list[index] -1)* jump_size\n",
    "            if cough_duration <= 0.5:\n",
    "                post_prediction_list[cough_start_list[index]:cough_end_list[index]] = [0]*(cough_end_list[index] - cough_start_list[index])\n",
    "                del cough_start_list[index]\n",
    "                del cough_end_list[index]\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "\n",
    "        if enable_IMU:\n",
    "            post_prediction_list_IMU = post_prediction_list.copy()\n",
    "            for index, value in enumerate(cough_start_list):\n",
    "                if min(distance_list[cough_start_list[index]:cough_end_list[index]]) > 4500:\n",
    "                    post_prediction_list_IMU[cough_start_list[index]:cough_end_list[index]] = [1] * (cough_end_list[index] - cough_start_list[index])\n",
    "                else:\n",
    "                    cough_piece = wav_chunk[int((cough_start_list[index] + 1) * sr * jump_size):int(\n",
    "                        (cough_end_list[index] - 1) * sr * jump_size + sr * win_size)]\n",
    "                    cough_duration = win_size + (cough_end_list[index] - cough_start_list[index] - 1) * jump_size\n",
    "                    cough_intensity = 20 * math.log10(np.sum(np.power(cough_piece * 2, 2))) / cough_duration\n",
    "                    cough_start_time = chunk_size * top_index + (cough_start_list[index] + 1) * jump_size\n",
    "                    cough_end_time = chunk_size * top_index + (cough_end_list[index] - 1) * jump_size + win_size\n",
    "                    cough_results_this = pd.DataFrame(\n",
    "                        {'start_time': [cough_start_time],\n",
    "                         'end_time': [cough_end_time],\n",
    "                         'duration': [cough_end_time - cough_start_time],\n",
    "                         'intensity': [cough_intensity],\n",
    "                         'type': ['dry']})\n",
    "                    final_cough_results = pd.concat([final_cough_results, cough_results_this])\n",
    "\n",
    "            prediction_dic = {'predictions': prediction_list, 'post_prediction': post_prediction_list, 'post_pred_IMU': post_prediction_list_IMU}\n",
    "        else:\n",
    "            prediction_dic = {'predictions': prediction_list, 'post_prediction': post_prediction_list}\n",
    "\n",
    "    else:\n",
    "        prediction_dic = {'predictions': prediction_list}\n",
    "\n",
    "    return prediction_dic, final_cough_results, distance_list, residual_wav, residual_IMU\n",
    "\n",
    "\n",
    "def audio_to_features(wav_chunk, sr, win_size, jump_size):\n",
    "    feature_table = pd.DataFrame()\n",
    "    for i in range(0, int((len(wav_chunk)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav_chunk):\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):]\n",
    "        else:\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "\n",
    "        feature_row_per_sub = wave_slice_to_device_features(wave_piece, sr)\n",
    "\n",
    "\n",
    "        if feature_table.empty:\n",
    "            feature_table = feature_row_per_sub\n",
    "        else:\n",
    "            feature_table = pd.concat([feature_table, feature_row_per_sub])\n",
    "\n",
    "    return feature_table.reset_index()\n",
    "\n",
    "def generate_opensmile_features_from_wav_sliding(wav, sr, win_size, jump_size):\n",
    "    # looping through the chunk for prediction\n",
    "    smile = opensmile.Smile(\n",
    "        feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "        feature_level=opensmile.FeatureLevel.Functionals, )\n",
    "    feature_table =pd.DataFrame()\n",
    "    for i in range(0, int((len(wav)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav):\n",
    "            wave_piece = wav[int(i * sr * jump_size):]\n",
    "        else:\n",
    "            wave_piece = wav[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "        sp.io.wavfile.write('temp.wav', sr, wave_piece.astype(np.int16))\n",
    "        wav_features_this = smile.process_file('temp.wav')\n",
    "        wav_features_this['piece_index'] = i\n",
    "        if feature_table.empty:\n",
    "            feature_table = wav_features_this\n",
    "        else:\n",
    "            feature_table = pd.concat([feature_table, wav_features_this])\n",
    "    return feature_table\n",
    "\n",
    "\n",
    "def process_audio_chunk(wav_chunk, sr, model, win_size, jump_size, post_processing, feature_list, silent_threshold):\n",
    "\n",
    "    prediction_list = []\n",
    "    distance_list = []\n",
    "\n",
    "    # looping through the chunk for prediction\n",
    "    for i in range(0, int((len(wav_chunk)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav_chunk):\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):]\n",
    "        else:\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "\n",
    "        feature_row_per_sub = wave_slice_to_device_features(wave_piece, sr)\n",
    "        if feature_row_per_sub['SPlevel'][0] < silent_threshold:\n",
    "            prediction_list.append(0)\n",
    "            distance_list.append(10000)\n",
    "        else:\n",
    "            selected_features = feature_row_per_sub[feature_list]\n",
    "            prediction = model.predict(selected_features)[0]\n",
    "\n",
    "            if prediction == 'cough':\n",
    "                prediction_list.append(2)\n",
    "\n",
    "            else:\n",
    "                prediction_list.append(1)\n",
    "                distance_list.append(10000)\n",
    "\n",
    "    final_cough_results = pd.DataFrame(columns=['start_time', 'end_time', 'duration', 'intensity', 'type'])\n",
    "    if post_processing:\n",
    "        index = 0\n",
    "        cough_start_list = []\n",
    "        cough_end_list = []\n",
    "        post_prediction_list = prediction_list.copy()\n",
    "        while index < (len(prediction_list[:-3])):\n",
    "            if prediction_list[index] == 2 and prediction_list[index + 3] == 2:\n",
    "                post_prediction_list[index + 1] = 2\n",
    "                post_prediction_list[index + 2] = 2\n",
    "                index += 4\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        index = 0\n",
    "        cough_happening = False\n",
    "        while index < (len(post_prediction_list)):\n",
    "            if post_prediction_list[index] == 2 and not cough_happening:\n",
    "                if index == (len(post_prediction_list) -1):\n",
    "                    cough_start_list.append(index)\n",
    "                    cough_end_list.append(index+1)\n",
    "                    break\n",
    "                else:\n",
    "                    cough_happening = True\n",
    "                    cough_start_list.append(index)\n",
    "                    index += 1\n",
    "                    continue\n",
    "            elif cough_happening and post_prediction_list[index] != 2:\n",
    "                cough_happening = False\n",
    "                cough_end_list.append(index)\n",
    "                index += 1\n",
    "                continue\n",
    "            elif cough_happening and index == (len(post_prediction_list)-1):\n",
    "                cough_end_list.append(index+1)\n",
    "                break\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        index = 0\n",
    "        while index < len(cough_start_list):\n",
    "            cough_duration = win_size + (cough_end_list[index] - cough_start_list[index] -1)* jump_size\n",
    "            if cough_duration <= 0.5:\n",
    "                post_prediction_list[cough_start_list[index]:cough_end_list[index]] = [0]*(cough_end_list[index] - cough_start_list[index])\n",
    "                del cough_start_list[index]\n",
    "                del cough_end_list[index]\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        prediction_dic = {'predictions': prediction_list, 'post_prediction': post_prediction_list}\n",
    "\n",
    "    else:\n",
    "        prediction_dic = {'predictions': prediction_list}\n",
    "\n",
    "    return prediction_dic, final_cough_results\n",
    "\n",
    "\n",
    "def process_IMU_chunk(IMU_sr, win_size, jump_size, IMU_DF, IMU_template_df):\n",
    "    j = 0\n",
    "    distance_acc = [600, 600, 600]\n",
    "    distance_gyr = [500, 500, 500]\n",
    "    distance_qua = [0.05, 0.05, 0.05]\n",
    "    time = [0, jump_size, 2*jump_size]\n",
    "    time_index = win_size/2\n",
    "    while j * jump_size * IMU_sr + IMU_sr* win_size < len(IMU_DF):\n",
    "        IMU_piece = IMU_DF.iloc[int(j * jump_size * IMU_sr):int(j * jump_size * IMU_sr + IMU_sr* win_size)]\n",
    "\n",
    "        alignment_L_X_a = dtw.distance(IMU_piece['ACC_X_f'].tolist(), IMU_template_df['acc_x'].tolist())\n",
    "        alignment_L_Y_a = dtw.distance(IMU_piece['ACC_Y_f'].tolist(), IMU_template_df['acc_y'].tolist())\n",
    "        alignment_L_Z_a = dtw.distance(IMU_piece['ACC_Z_f'].tolist(), IMU_template_df['acc_z'].tolist())\n",
    "        distance_acc.append((alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(3)))\n",
    "\n",
    "        alignment_L_X_a = dtw.distance(IMU_piece['GYR_X_f'].tolist(), IMU_template_df['gyr_x'].tolist())\n",
    "        alignment_L_Y_a = dtw.distance(IMU_piece['GYR_Y_f'].tolist(), IMU_template_df['gyr_y'].tolist())\n",
    "        alignment_L_Z_a = dtw.distance(IMU_piece['GYR_Z_f'].tolist(), IMU_template_df['gyr_z'].tolist())\n",
    "        distance_gyr.append((alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(3)))\n",
    "\n",
    "        alignment_L_X_a = dtw.distance(IMU_piece['QUA_X_f'].tolist(), IMU_template_df['qua_x'].tolist())\n",
    "        alignment_L_Y_a = dtw.distance(IMU_piece['QUA_Y_f'].tolist(), IMU_template_df['qua_y'].tolist())\n",
    "        alignment_L_Z_a = dtw.distance(IMU_piece['QUA_Z_f'].tolist(), IMU_template_df['qua_z'].tolist())\n",
    "        distance_qua.append((alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(3)))\n",
    "\n",
    "        time_index += jump_size\n",
    "        time.append(time_index)\n",
    "        j += 1\n",
    "\n",
    "    distance_df = pd.DataFrame({'distance_acc': distance_acc, 'distance_gyr': distance_gyr, 'distance_qua': distance_qua}, index=time)\n",
    "\n",
    "    distance_df['distance_IMU'] = distance_df['distance_acc']* distance_df['distance_gyr']* distance_df['distance_qua']\n",
    "\n",
    "    return distance_df\n",
    "\n",
    "\n",
    "def process_the_chunk_train(wav_chunk, sr, IMU_chunk, enable_IMU, residual_wav, residual_IMU, model, win_size, jump_size, post_processing, save_cough_snippets, feature_list):\n",
    "\n",
    "\n",
    "    if enable_IMU:\n",
    "        # smoothing the ACC\n",
    "        IMU_acc_L_x = IMU_chunk[' L_ACC_X'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_L_y = IMU_chunk[' L_ACC_Y'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_L_z = IMU_chunk[' L_ACC_Z'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_L_x = ap.butter_filter(IMU_acc_L_x, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_L_y = ap.butter_filter(IMU_acc_L_y, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_L_z = ap.butter_filter(IMU_acc_L_z, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_chunk[' L_ACC_X_f'] = IMU_acc_L_x\n",
    "        IMU_chunk[' L_ACC_Y_f'] = IMU_acc_L_y\n",
    "        IMU_chunk[' L_ACC_Z_f'] = IMU_acc_L_z\n",
    "        IMU_acc_R_x = IMU_chunk[' R_ACC_X'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_R_y = IMU_chunk[' R_ACC_Y'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_R_z = IMU_chunk[' R_ACC_Z'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_R_x = ap.butter_filter(IMU_acc_R_x, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_R_y = ap.butter_filter(IMU_acc_R_y, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_R_z = ap.butter_filter(IMU_acc_R_z, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_chunk[' R_ACC_X_f'] = IMU_acc_R_x\n",
    "        IMU_chunk[' R_ACC_Y_f'] = IMU_acc_R_y\n",
    "        IMU_chunk[' R_ACC_Z_f'] = IMU_acc_R_z\n",
    "\n",
    "    prediction_list = []\n",
    "    distance_list = []\n",
    "    wav_chunk = np.concatenate((residual_wav, wav_chunk), axis=None)\n",
    "\n",
    "    IMU_chunk = pd.concat([residual_IMU, IMU_chunk])\n",
    "    # looping through the chunk for prediction\n",
    "    for i in range(0, int((len(wav_chunk)) / (sr * jump_size))):\n",
    "        if int(i * sr * jump_size + sr * win_size) > len(wav_chunk):\n",
    "            residual_wav = wav_chunk[int(i * sr * jump_size + sr * win_size):]\n",
    "            residual_IMU = IMU_chunk.iloc[int(i * 80 * jump_size + 80 * win_size):]\n",
    "            break\n",
    "        else:\n",
    "            wave_piece = wav_chunk[int(i * sr * jump_size):int(i * sr * jump_size + sr * win_size)]\n",
    "            feature_row_per_sub = wave_slice_to_device_features(wave_piece, sr)\n",
    "            if feature_row_per_sub['SPlevel'][0] < -66:\n",
    "                prediction_list.append(0)\n",
    "                distance_list.append(10000)\n",
    "            else:\n",
    "                selected_features = feature_row_per_sub[feature_list]\n",
    "                prediction = model.predict(selected_features)[0]\n",
    "                # if prediction == 'cough':\n",
    "                #\n",
    "                #     if enable_IMU:\n",
    "                #         IMU_piece = IMU_chunk.iloc[int(i * 80 * jump_size):int(i * 80 * jump_size + 50)]\n",
    "                #         alignment_R_X_a = dtw.distance(IMU_piece[' R_ACC_X_f'].tolist(), template_R_X_a.tolist())\n",
    "                #         alignment_R_Y_a = dtw.distance(IMU_piece[' R_ACC_Y_f'].tolist(), template_R_Y_a.tolist())\n",
    "                #         alignment_R_Z_a = dtw.distance(IMU_piece[' R_ACC_Z_f'].tolist(), template_R_Z_a.tolist())\n",
    "                #         alignment_L_X_a = dtw.distance(IMU_piece[' L_ACC_X_f'].tolist(), template_L_X_a.tolist())\n",
    "                #         alignment_L_Y_a = dtw.distance(IMU_piece[' L_ACC_Y_f'].tolist(), template_L_Y_a.tolist())\n",
    "                #         alignment_L_Z_a = dtw.distance(IMU_piece[' L_ACC_Z_f'].tolist(), template_L_Z_a.tolist())\n",
    "                #         distance = (alignment_R_X_a * alignment_R_Y_a * alignment_R_Z_a * alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(6))\n",
    "                if prediction == 'cough':\n",
    "                    prediction_list.append(2)\n",
    "                    if enable_IMU:\n",
    "                        IMU_piece = IMU_chunk.iloc[int(i * 80 * jump_size):int(i * 80 * jump_size + 50)]\n",
    "                        alignment_R_X_a = dtw.distance(IMU_piece[' R_ACC_X_f'].tolist(), template_R_X_a.tolist())\n",
    "                        alignment_R_Y_a = dtw.distance(IMU_piece[' R_ACC_Y_f'].tolist(), template_R_Y_a.tolist())\n",
    "                        alignment_R_Z_a = dtw.distance(IMU_piece[' R_ACC_Z_f'].tolist(), template_R_Z_a.tolist())\n",
    "                        alignment_L_X_a = dtw.distance(IMU_piece[' L_ACC_X_f'].tolist(), template_L_X_a.tolist())\n",
    "                        alignment_L_Y_a = dtw.distance(IMU_piece[' L_ACC_Y_f'].tolist(), template_L_Y_a.tolist())\n",
    "                        alignment_L_Z_a = dtw.distance(IMU_piece[' L_ACC_Z_f'].tolist(), template_L_Z_a.tolist())\n",
    "                        distance = (alignment_R_X_a * alignment_R_Y_a * alignment_R_Z_a * alignment_L_X_a * alignment_L_Y_a * alignment_L_Z_a) ** (1 / float(6))\n",
    "                        distance_list.append(distance)\n",
    "                else:\n",
    "                    prediction_list.append(1)\n",
    "                    distance_list.append(10000)\n",
    "\n",
    "    if post_processing:\n",
    "        index = 0\n",
    "        cough_start_list = []\n",
    "        cough_end_list = []\n",
    "        post_prediction_list = prediction_list.copy()\n",
    "        final_cough_results = pd.DataFrame(columns = ['start_index' , 'end_index', 'duration'])\n",
    "        while index < (len(prediction_list[:-3])):\n",
    "            if prediction_list[index] == 2 and prediction_list[index + 3] == 2:\n",
    "                post_prediction_list[index + 1] = 2\n",
    "                post_prediction_list[index + 2] = 2\n",
    "                index += 4\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        index = 0\n",
    "        cough_happening = False\n",
    "        while index < (len(post_prediction_list)):\n",
    "            if post_prediction_list[index] == 2 and not cough_happening:\n",
    "                if index == (len(post_prediction_list) -1):\n",
    "                    cough_start_list.append(index)\n",
    "                    cough_end_list.append(index+1)\n",
    "                    break\n",
    "                else:\n",
    "                    cough_happening = True\n",
    "                    cough_start_list.append(index)\n",
    "                    index += 1\n",
    "                    continue\n",
    "            elif cough_happening and post_prediction_list[index] != 2:\n",
    "                cough_happening = False\n",
    "                cough_end_list.append(index)\n",
    "                index += 1\n",
    "                continue\n",
    "            elif cough_happening and index == (len(post_prediction_list)-1):\n",
    "                cough_end_list.append(index+1)\n",
    "                break\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        for index, value in enumerate(cough_start_list):\n",
    "            cough_duration = win_size + (cough_end_list[index] - cough_start_list[index] -1)* jump_size\n",
    "            if cough_duration <= 0.6:\n",
    "                post_prediction_list[cough_start_list[index]:cough_end_list[index]] = [0]*(cough_end_list[index] - cough_start_list[index])\n",
    "            else:\n",
    "                final_cough_results.append({'start_index': cough_start_list[index], 'end_index': cough_start_list[index],\n",
    "                                            'duration': cough_duration}, ignore_index=True)\n",
    "\n",
    "        if enable_IMU:\n",
    "            post_prediction_list_IMU = post_prediction_list.copy()\n",
    "            for index, value in enumerate(cough_start_list):\n",
    "                if min(distance_list[cough_start_list[index]:cough_end_list[index]]) > 4500:\n",
    "                    post_prediction_list_IMU[cough_start_list[index]:cough_end_list[index]] = [1] * (cough_end_list[index] - cough_start_list[index])\n",
    "            prediction_dic = {'predictions': prediction_list, 'post_prediction': post_prediction_list, 'post_pred_IMU': post_prediction_list_IMU}\n",
    "        else:\n",
    "            prediction_dic = {'predictions': prediction_list, 'post_prediction': post_prediction_list}\n",
    "\n",
    "    else:\n",
    "        prediction_dic = {'predictions': prediction_list}\n",
    "\n",
    "    return prediction_dic, distance_list, residual_wav, residual_IMU\n",
    "\n",
    "def IMU_DTW_train(IMU_DF, label_DF):\n",
    "\n",
    "    IMU_start_time_L = float(IMU_DF.iloc[0, 0])\n",
    "    IMU_start_time_R = float(IMU_DF.iloc[0, 7])\n",
    "\n",
    "    IMU_time_offset = 1000\n",
    "    IMU_piece_list = []\n",
    "    alex_cough_index = [0, 1, 2, 4, 5, 6, 7, 9, 10, 14, 15, 16]\n",
    "    alex_cough_timing = {\n",
    "        0: '670-690',\n",
    "        1: '1225-1240',\n",
    "        2: '1550-1570',\n",
    "        4: '2220-2240',\n",
    "        5: '2550-2570',\n",
    "        6: '2860-2880',\n",
    "        7: '3123-3143',\n",
    "        8: '3410-3435',\n",
    "        9: '3665-3685',\n",
    "        10: '3987-4007',\n",
    "        14: '5252-5272',\n",
    "        15: '5595-5615',\n",
    "        16: '5905-5925'\n",
    "    }\n",
    "    tousif_cough_index = [1, 3, 5, 7, 17, 21, 23]\n",
    "    tousif_cough_timing={\n",
    "        1: '147-166',\n",
    "        3: '395-414',\n",
    "        5: '788-804',\n",
    "        7: '1115-1135',\n",
    "        17: '3332-3356',\n",
    "        21: '4333-4347',\n",
    "        23: '4730-4747'\n",
    "    }\n",
    "    wenchuan_cough_index = [0, 1, 2, 6, 9, 10]\n",
    "    wenchuan_cough_timing = {\n",
    "        0: '962-982',\n",
    "        1: '1320-1340',\n",
    "        2: '1582-1603',\n",
    "        6: '2708-2722',\n",
    "        9: '3669-3685',\n",
    "        10: '4395-4412'\n",
    "    }\n",
    "    wenchuan_new_cough_index = [0, 1, 2, 6, 9, 10]\n",
    "    wenchuan_new_cough_timing = {\n",
    "        0: '962-982',\n",
    "        1: '1320-1340',\n",
    "        2: '1582-1603',\n",
    "        6: '2708-2722',\n",
    "        9: '3669-3685',\n",
    "        10: '4395-4412'\n",
    "    }\n",
    "    roy_cough_index = [1, 2, 8, 9, 10, 11, 15]\n",
    "    roy_cough_timing = {\n",
    "        1: '317-336',\n",
    "        2: '348-364',\n",
    "        8: '1688-1707',\n",
    "        9: '1723-1738',\n",
    "        10: '2403-2421',\n",
    "        11: '2438-2457',\n",
    "        15: '3973-3990'\n",
    "    }\n",
    "    jay_cough_index = [0, 3, 4, 7, 9, 12, 14, 15, 16, 22, 24, 25]\n",
    "    # the timing for jay is setup based on IMU_time_offset = 300\n",
    "    jay_cough_timing = {\n",
    "        0: '346-366',\n",
    "        3: '730-750',\n",
    "        4: '775-797',\n",
    "        7: '1265-1292',\n",
    "        9: '1815-1835',\n",
    "        12: '2314-2333',\n",
    "        14: '2412-2438',\n",
    "        15: '2930-2947',\n",
    "        16: '2963-2985',\n",
    "        22: '4075-4100',\n",
    "        24: '4688-4705',\n",
    "        25: '4720-4740'\n",
    "    }\n",
    "    cough_timing = alex_cough_timing\n",
    "    for row_index, label_row in label_DF.iterrows():\n",
    "        if row_index not in alex_cough_index:\n",
    "            continue\n",
    "\n",
    "        if label_row.label == 'cough':\n",
    "            cough_start = float(label_row.start_times)\n",
    "            cough_stop = float(label_row.stop_times)\n",
    "            print(cough_start, cough_stop)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        IMU_piece = IMU_DF[IMU_DF[' R_UPTIME'] >= (IMU_start_time_R + cough_start * 1000 - IMU_time_offset)]\n",
    "        IMU_piece = IMU_piece[IMU_piece[' R_UPTIME'] < (IMU_start_time_R + cough_stop * 1000 + IMU_time_offset)]\n",
    "\n",
    "        ACC_piece_L = IMU_piece[[' L_ACC_X_f', ' L_ACC_Y_f', ' L_ACC_Z_f']]\n",
    "        ACC_piece_R = IMU_piece[[' R_ACC_X_f', ' R_ACC_Y_f', ' R_ACC_Z_f']]\n",
    "        ACC_piece_L = ACC_piece_L - ACC_piece_L.mean()\n",
    "        ACC_piece_R = ACC_piece_R - ACC_piece_R.mean()\n",
    "\n",
    "        # # focusing on exactly the signature part\n",
    "        ACC_piece_L = ACC_piece_L.loc[int(cough_timing[row_index].split('-')[0]):int(cough_timing[row_index].split('-')[1])]\n",
    "        ACC_piece_R = ACC_piece_R.loc[int(cough_timing[row_index].split('-')[0]):int(cough_timing[row_index].split('-')[1])]\n",
    "        print(ACC_piece_L.shape)\n",
    "\n",
    "        b = signal.resample(ACC_piece_R[' R_ACC_X_f'].values, 20)\n",
    "        IMU_piece_list.append(b)\n",
    "        plt.plot(b)\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "        ACC_piece_L.plot(ax=axes[0])\n",
    "        ACC_piece_R.plot(ax=axes[1])\n",
    "        plt.savefig('figures/Alex/' + str(row_index) + '.png')\n",
    "\n",
    "\n",
    "    plt.savefig('0_IMU_piece.png')\n",
    "    plt.close()\n",
    "    template = sum(IMU_piece_list) / len(IMU_piece_list)\n",
    "    plt.plot(template)\n",
    "    plt.savefig('0_template.png')\n",
    "    plt.close()\n",
    "    # pickle.dump(template, open('templates/wenchuan/template_L_X_a_f', 'wb'))\n",
    "    np.savetxt('templates/Alex/focused/template_R_X_a_f_hfp.csv', template, delimiter=\",\")\n",
    "\n",
    "\n",
    "def visualize_audio_IMU(subject, task_id, IMU_version, sensor, filter_IMU, normalize_wav, IMU_axis):\n",
    "    import sync_func\n",
    "    subject_path = database_path + subject + '/'\n",
    "    data_path = subject_path + allTasks_dic[task_id]\n",
    "    print(data_path)\n",
    "    audio_file = list_files(data_path, 'wav')[0]\n",
    "\n",
    "    sr_test, wav_test = sp.io.wavfile.read(audio_file)\n",
    "    # loading the IMU file\n",
    "    if IMU_version == 'raw':\n",
    "        IMU_path = list_files(subject_path + allTasks_dic[task_id], 'imu.csv')[0]\n",
    "        IMU_DF = pd.read_csv(IMU_path)\n",
    "    elif (IMU_version == 'syn') or (IMU_version == 'man'):\n",
    "        try:\n",
    "            IMU_DF, drift_sec = sync_func.load_sync_imu(database_path, subject, allTasks_dic[task_id], sync_file, 50)\n",
    "        except:\n",
    "            print('the IMU file is not synched for', subject, task_id)\n",
    "    else:\n",
    "        print(\"the chosen IMU_version is not valid\")\n",
    "\n",
    "    if filter_IMU == 'HPF':\n",
    "        IMU_acc_x = IMU_DF['v.accX'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_y = IMU_DF['v.accY'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_z = IMU_DF['v.accZ'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_acc_x = ap.butter_filter(IMU_acc_x, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_y = ap.butter_filter(IMU_acc_y, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_acc_z = ap.butter_filter(IMU_acc_z, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_DF['ACC_X_f'] = IMU_acc_x\n",
    "        IMU_DF['ACC_Y_f'] = IMU_acc_y\n",
    "        IMU_DF['ACC_Z_f'] = IMU_acc_z\n",
    "        IMU_gyr_x = IMU_DF['v.gyroX'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_gyr_y = IMU_DF['v.gyroY'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_gyr_z = IMU_DF['v.gyroZ'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_gyr_x = ap.butter_filter(IMU_gyr_x, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_gyr_y = ap.butter_filter(IMU_gyr_y, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_gyr_z = ap.butter_filter(IMU_gyr_z, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_DF['GYR_X_f'] = IMU_gyr_x\n",
    "        IMU_DF['GYR_Y_f'] = IMU_gyr_y\n",
    "        IMU_DF['GYR_Z_f'] = IMU_gyr_z\n",
    "        IMU_qua_w = IMU_DF['v.qw'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_qua_x = IMU_DF['v.qx'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_qua_y = IMU_DF['v.qy'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_qua_z = IMU_DF['v.qz'].rolling(window=10, min_periods=1).mean().values\n",
    "        IMU_qua_w = ap.butter_filter(IMU_qua_w, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_qua_x = ap.butter_filter(IMU_qua_x, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_qua_y = ap.butter_filter(IMU_qua_y, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_qua_z = ap.butter_filter(IMU_qua_z, 45, 0.3, btype=\"high\", order=2)\n",
    "        IMU_DF['QUA_W_f'] = IMU_qua_w\n",
    "        IMU_DF['QUA_X_f'] = IMU_qua_x\n",
    "        IMU_DF['QUA_Y_f'] = IMU_qua_y\n",
    "        IMU_DF['QUA_Z_f'] = IMU_qua_z\n",
    "\n",
    "    elif filter_IMU == 'EMD':\n",
    "        imu_data = sync_func.resample(IMU_DF, \"0.02S\", \"ts\")  # 50 Hz\n",
    "        s = imu_data[\"v.accZ\"].values\n",
    "        t = imu_data[\"ts\"].values\n",
    "        t = (t - t[0]) / 1000\n",
    "        imu = sync_func.emd_1st_component(s, t)\n",
    "        plt.plot(imu)\n",
    "        plt.show()\n",
    "        exit()\n",
    "\n",
    "        # =================================================================================\n",
    "        # use Envelope of SPlevel and IMF1 to replace SPlevel and IMF1\n",
    "        # =================================================================================\n",
    "        # SPlevel = butter_filter(SPlevel, 50, 1.5, btype=\"low\", order=2)\n",
    "        # from scipy.signal import hilbert\n",
    "        # SPlevel = np.abs(hilbert(SPlevel))\n",
    "        # imu = np.abs(hilbert(imu))\n",
    "\n",
    "        # drift, shift, fftshift = sync_func.sync(SPlevel, imu, win)\n",
    "        # if plot:\n",
    "        #     plt_three_figures(SPlevel[:min(imu.shape[0], SPlevel.shape[0])],\n",
    "        #                       imu[:min(imu.shape[0], SPlevel.shape[0])],\n",
    "        #                       fftshift,\n",
    "        #                       drift)\n",
    "        # print(\"drift (ms)\", drift)\n",
    "    else:\n",
    "        if filter_IMU != 'None':\n",
    "            print(\"method must be either EMD or HPF\")\n",
    "\n",
    "    # normalizing the amplitude\n",
    "    if normalize_wav:\n",
    "        max_wav = 32800\n",
    "        # expanding the resolution to float so it doesn't go out of range if wav is 16 bit int\n",
    "        wav_test = wav_test.astype(int)\n",
    "        normalization_ratio = max_wav * 2\n",
    "        wav_test = (wav_test + max_wav) / normalization_ratio - 0.5\n",
    "        # rounding the digits after decimal point to a certain number\n",
    "        wav_test = np.round(wav_test, decimals=5)\\\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.plot(wav_test)\n",
    "    ax1.set_title('Audio')\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.plot((IMU_DF['ts'] - int(IMU_DF['ts'].iloc[0])).tolist(), IMU_DF[sensor + '_' + IMU_axis + '_f'].tolist())\n",
    "    ax2.set_title('IMU')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def pcm_to_wav(file_path, nchannels, nbits, sr):\n",
    "    with open(file_path, 'rb') as pcmfile:\n",
    "        pcmdata = pcmfile.read()\n",
    "    with wave.open(file_path[:-4]+'.wav', 'wb') as wavfile:\n",
    "        wavfile.setnchannels(nchannels)\n",
    "        wavfile.setsampwidth(int(nbits/8))\n",
    "        wavfile.setframerate(sr)\n",
    "        wavfile.setnframes(0)\n",
    "        wavfile.writeframes(pcmdata)\n",
    "\n",
    "def wave_slice_to_device_features(slice, sr, label= None, silence_remover= False, silence_thresh = -80):\n",
    "\n",
    "\n",
    "    #columns = ['F1', 'F2', 'F3', 'F4']\n",
    "\n",
    "    magnitudes = np.abs(np.fft.rfft(slice))\n",
    "    freqs = np.abs(np.fft.fftfreq(len(slice), 1.0 / sr)[:len(slice) // 2 + 1])\n",
    "    wave_instance = Device_Features(slice.astype(float),sr, magnitudes, freqs)\n",
    "\n",
    "    # delete the old temp.csv file\n",
    "    if os.path.exists(\"temp.csv\"):\n",
    "        os.remove(\"temp.csv\")\n",
    "\n",
    "    feature_row = []\n",
    "    if len(slice) != 0:\n",
    "\n",
    "        feature_row.append(len(slice)/float(sr))\n",
    "        feature_row.append(np.mean(abs(slice)))                         # mean\n",
    "        feature_row.append(np.median(abs(slice)))                       # median\n",
    "        feature_row.append(np.std(slice, ddof=1))                       # std\n",
    "        feature_row.append(sp.stats.skew(slice))                        # skewness\n",
    "        feature_row.append(sp.stats.kurtosis(slice))                    # kurtosis\n",
    "        feature_row.append(wave_instance.zcr())                         # zcr\n",
    "        feature_row.append(wave_instance.SPlevel())                     # spLevel\n",
    "        feature_row.append(wave_instance.Quartile_Range())              # Quartile Range\n",
    "        feature_row.append(wave_instance.Spec_Cent())                   # spectral Centroid\n",
    "        feature_row.append(wave_instance.Spec_Spread())                 # spectral spread\n",
    "        feature_row.append(wave_instance.Spec_Rolloff())                # spectral RollOff\n",
    "        feature_row.append(wave_instance.Spec_Flatness())               # spectral flatness\n",
    "        feature_row.append(sp.stats.skew(magnitudes))                   # spectral skewness\n",
    "        feature_row.append(sp.stats.kurtosis(magnitudes))               # spectral kurtosis\n",
    "        feature_row.append(np.std(magnitudes, ddof=1))                  # Spectral Standard Deviation (SSD)\n",
    "        feature_row = feature_row + wave_instance.Chroma()              # chromas\n",
    "        # mfcc = wave_instance.MFCC()\n",
    "        # mfcc = speech_feat.mfcc(slice, sr, winlen=len(slice), winstep=len(slice), numcep=40)\n",
    "        # mfcc = speech_feat.mfcc(slice, samplerate=sr, winlen=0.4, winstep=0.01, numcep=20, nfilt=26, nfft=16384)\n",
    "        mfcc = speech_feat.mfcc(slice, samplerate=sr, winlen=0.4, winstep=0.01, numcep=20, nfilt=26, nfft=6400, preemph=0)\n",
    "\n",
    "        # mfcc_2 = speech_feat.mfcc(slice, samplerate=sr, winlen=0.025, winstep=0.01, numcep=20, nfilt=26, nfft=1024)\n",
    "        #\n",
    "        # mfcc_2_ave = np.average(mfcc_2, axis=0).reshape((1,20))\n",
    "        # mfcc_2_std = np.std(mfcc_2, axis=0, ddof=1).reshape((1, 20))\n",
    "\n",
    "        feature_row = feature_row + mfcc.tolist()[0]\n",
    "        # feature_row = feature_row + mfcc.tolist()[0] + mfcc_2_ave.tolist()[0] + mfcc_2_std.tolist()[0]\n",
    "\n",
    "    else:\n",
    "        print (slice, \"slice is empty\")\n",
    "\n",
    "#     feature_table = pd.DataFrame([feature_row])\n",
    "#     if silence_remover:\n",
    "#         if float(feature_table[\"SPlevel\"])  < silence_thresh:\n",
    "#             feature_table['label'] = \"silent\"\n",
    "#         else:\n",
    "#             feature_table['label'] = label\n",
    "#     else:\n",
    "#         feature_table['label'] = label\n",
    "    return feature_row\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1a84ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(1, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(2, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(2, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(2, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n",
      "(3, 48)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Retrieve labels for this file\u001b[39;00m\n\u001b[1;32m     55\u001b[0m file_labels \u001b[38;5;241m=\u001b[39m labels_df[labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m file_name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 57\u001b[0m features  \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     59\u001b[0m all_features\u001b[38;5;241m.\u001b[39mextend(features)\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(segments, sr)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(segment) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m         feat \u001b[38;5;241m=\u001b[39m \u001b[43mwave_slice_to_device_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#         mfcc_mean = np.mean(mfcc, axis=1)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         features\u001b[38;5;241m.\u001b[39mappend(feat)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mwave_slice_to_device_features\u001b[0;34m(slice, sr, label, silence_remover, silence_thresh)\u001b[0m\n\u001b[1;32m   1013\u001b[0m feature_row\u001b[38;5;241m.\u001b[39mappend(wave_instance\u001b[38;5;241m.\u001b[39mSpec_Spread())                 \u001b[38;5;66;03m# spectral spread\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m feature_row\u001b[38;5;241m.\u001b[39mappend(wave_instance\u001b[38;5;241m.\u001b[39mSpec_Rolloff())                \u001b[38;5;66;03m# spectral RollOff\u001b[39;00m\n\u001b[0;32m-> 1015\u001b[0m feature_row\u001b[38;5;241m.\u001b[39mappend(\u001b[43mwave_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpec_Flatness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)               \u001b[38;5;66;03m# spectral flatness\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m feature_row\u001b[38;5;241m.\u001b[39mappend(sp\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mskew(magnitudes))                   \u001b[38;5;66;03m# spectral skewness\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m feature_row\u001b[38;5;241m.\u001b[39mappend(sp\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mkurtosis(magnitudes))               \u001b[38;5;66;03m# spectral kurtosis\u001b[39;00m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mDevice_Features.Spec_Flatness\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m magnitudes:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m         num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m         den \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m den \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# File paths\n",
    "wav_files = glob.glob(\"ComParE2020_Mask/wav/*.wav\")\n",
    "label_file = 'Mask_labels_confidential.csv'\n",
    "\n",
    "# Read the label file\n",
    "labels_df = pd.read_csv(label_file)\n",
    "\n",
    "# Function to segment audio files\n",
    "def segment_audio(file_path, segment_length=0.5, overlap=0.1):\n",
    "#     sr, y = wavfile.read(file_path)\n",
    "    \n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    y, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "    segment_length_samples = int(segment_length * sr)\n",
    "    overlap_samples = int(overlap * sr)\n",
    "    segments = []\n",
    "\n",
    "    for start in range(0, len(y), segment_length_samples - overlap_samples):\n",
    "        end = min(start + segment_length_samples, len(y))\n",
    "        segments.append(y[start:end])\n",
    "\n",
    "    return segments, sr\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_features(segments, sr):\n",
    "    features = []\n",
    "    for segment in segments:\n",
    "        if len(segment) == 0:\n",
    "            continue\n",
    "        feat = wave_slice_to_device_features(segment, sr=sr)\n",
    "#         mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "# Process each audio file and extract features\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for file_path in wav_files:\n",
    "    segments, sr = segment_audio(file_path)\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Retrieve labels for this file\n",
    "    file_labels = labels_df[labels_df['file_name'] == file_name]['label'].values\n",
    "\n",
    "    features  = extract_features(segments, sr)\n",
    "    print(features.shape)\n",
    "    all_features.extend(features)\n",
    "    labels = []\n",
    "    for i in range(len(features)):\n",
    "        labels.append(file_labels[0])\n",
    "        \n",
    "    all_labels.extend(labels)\n",
    "\n",
    "all_features = np.array(all_features)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Checking the shapes of the features and labels\n",
    "all_features.shape, all_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391d6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('SRA_features.npy', all_features)\n",
    "np.save('SRA_labels.npy', all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34efcc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.array(all_features)\n",
    "# X = np.reshape(X, [X.shape[0],49])\n",
    "# y = np.array(labels_list)\n",
    "\n",
    "# X_ = X[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c223db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071f8649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59128, 48)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8a1dea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train a RandomForest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf30d3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.71%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c680b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       clear       0.61      0.58      0.59      5715\n",
      "        mask       0.62      0.65      0.64      6111\n",
      "\n",
      "    accuracy                           0.62     11826\n",
      "   macro avg       0.62      0.62      0.62     11826\n",
      "weighted avg       0.62      0.62      0.62     11826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed8baf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes = np.unique(y_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Plot confusion matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8740afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.60976512 0.62330581]\n",
      "recall: [0.57690289 0.65472099]\n",
      "fscore: [0.59287898 0.63862729]\n",
      "support: [5715 6111]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(y_test, y_pred)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee18ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0a260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
